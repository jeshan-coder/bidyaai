{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX_F5Zz8Xial",
        "outputId": "06656629-26fe-472e-fca7-466c29688ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=5a0546956584c4920be1fa061ddba8af708526181a3d26d96d9b74839fed5d8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas nltk rouge-score scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- Custom Functions for Metrics Calculation ---\n",
        "\n",
        "def calculate_exact_match(ground_truth, predicted):\n",
        "    \"\"\"Calculates if the predicted response is an exact match.\"\"\"\n",
        "    return ground_truth.strip() == predicted.strip()\n",
        "\n",
        "def calculate_bleu(ground_truth, predicted):\n",
        "    \"\"\"Calculates a simple BLEU score using a basic tokenizer.\"\"\"\n",
        "    reference = ground_truth.lower().split()\n",
        "    candidate = predicted.lower().split()\n",
        "\n",
        "    if not candidate:\n",
        "        return 0.0\n",
        "\n",
        "    candidate_counts = Counter(candidate)\n",
        "    reference_counts = Counter(reference)\n",
        "\n",
        "    clipped_counts = {word: min(candidate_counts[word], reference_counts.get(word, 0)) for word in candidate_counts}\n",
        "    precision = sum(clipped_counts.values()) / sum(candidate_counts.values()) if sum(candidate_counts.values()) > 0 else 0.0\n",
        "\n",
        "    brevity_penalty = 1.0\n",
        "    if len(candidate) < len(reference):\n",
        "        brevity_penalty = np.exp(1 - len(reference) / len(candidate)) if len(candidate) > 0 else 0.0\n",
        "\n",
        "    return brevity_penalty * precision\n",
        "\n",
        "\n",
        "def calculate_rouge(ground_truth, predicted):\n",
        "    \"\"\"Calculates ROUGE-like scores manually without external libraries.\"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    def get_ngrams(text, n):\n",
        "        words = text.lower().split()\n",
        "        return Counter(tuple(words[i:i+n]) for i in range(len(words) - n + 1))\n",
        "\n",
        "    def calculate_f1(pred_ngrams, ref_ngrams):\n",
        "        if not pred_ngrams:\n",
        "            return 0.0\n",
        "\n",
        "        overlap = sum(min(pred_ngrams[ngram], ref_ngrams.get(ngram, 0)) for ngram in pred_ngrams)\n",
        "        precision = overlap / sum(pred_ngrams.values()) if sum(pred_ngrams.values()) > 0 else 0.0\n",
        "        recall = overlap / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    # ROUGE-L (Longest Common Subsequence)\n",
        "    def calculate_rouge_l(pred_words, ref_words):\n",
        "        m, n = len(pred_words), len(ref_words)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                if pred_words[i] == ref_words[j]:\n",
        "                    dp[i+1][j+1] = dp[i][j] + 1\n",
        "                else:\n",
        "                    dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n",
        "        lcs = dp[m][n]\n",
        "        precision = lcs / m if m > 0 else 0\n",
        "        recall = lcs / n if n > 0 else 0\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    ground_truth_words = ground_truth.lower().split()\n",
        "    predicted_words = predicted.lower().split()\n",
        "\n",
        "    rouge1_f1 = calculate_f1(get_ngrams(ground_truth, 1), get_ngrams(predicted, 1))\n",
        "    rouge2_f1 = calculate_f1(get_ngrams(ground_truth, 2), get_ngrams(predicted, 2))\n",
        "    rougel_f1 = calculate_rouge_l(ground_truth_words, predicted_words)\n",
        "\n",
        "    return {\n",
        "        'rouge1_f1': rouge1_f1,\n",
        "        'rouge2_f1': rouge2_f1,\n",
        "        'rougel_f1': rougel_f1\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(ground_truth_list, predicted_list):\n",
        "    \"\"\"\n",
        "    Calculates cosine similarity for all pairs using TF-IDF.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(token_pattern=r'\\b\\w+\\b').fit(ground_truth_list + predicted_list)\n",
        "    gt_vectors = vectorizer.transform(ground_truth_list)\n",
        "    pred_vectors = vectorizer.transform(predicted_list)\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(ground_truth_list)):\n",
        "        sim = cosine_similarity(gt_vectors[i:i+1], pred_vectors[i:i+1])\n",
        "        similarities.append(sim[0][0])\n",
        "    return similarities\n",
        "\n",
        "# --- 1. Load the data ---\n",
        "try:\n",
        "    df = pd.read_csv(\"final_results.csv\")\n",
        "    print(\"\\nData loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nError: 'final_results.csv' not found. Please ensure the file is in the same directory as the script.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Apply the metrics and store results ---\n",
        "print(\"Calculating per-row metrics...\")\n",
        "metrics_df = pd.DataFrame()\n",
        "metrics_df['class'] = df['class']\n",
        "metrics_df['subject'] = df['subject']\n",
        "metrics_df['Exact_Match'] = df.apply(\n",
        "    lambda row: calculate_exact_match(row['Ground Truth Response'], row['Predicted Response']),\n",
        "    axis=1\n",
        ").astype(int)\n",
        "metrics_df['BLEU'] = df.apply(\n",
        "    lambda row: calculate_bleu(row['Ground Truth Response'], row['Predicted Response']),\n",
        "    axis=1\n",
        ")\n",
        "rouge_scores = df.apply(\n",
        "    lambda row: calculate_rouge(row['Ground Truth Response'], row['Predicted Response']),\n",
        "    axis=1\n",
        ").tolist()\n",
        "rouge_df = pd.DataFrame(rouge_scores)\n",
        "metrics_df = pd.concat([metrics_df, rouge_df], axis=1)\n",
        "metrics_df['Cosine_Similarity'] = calculate_cosine_similarity(\n",
        "    df['Ground Truth Response'].fillna('').tolist(),\n",
        "    df['Predicted Response'].fillna('').tolist()\n",
        ")\n",
        "\n",
        "# --- 3. Calculate and save overall metrics CSVs based on user request ---\n",
        "metrics_columns = ['Exact_Match', 'BLEU', 'rouge1_f1', 'rouge2_f1', 'rougel_f1', 'Cosine_Similarity']\n",
        "csv_files_to_zip = []\n",
        "\n",
        "# Group metrics by class and subject\n",
        "class_subject_metrics = metrics_df.groupby(['class', 'subject'])[metrics_columns].mean().reset_index()\n",
        "\n",
        "# Save a separate CSV for each class with its subject metrics\n",
        "print(\"\\nSaving metrics for each class (subject-wise breakdown)...\")\n",
        "for cls in class_subject_metrics['class'].unique():\n",
        "    class_df = class_subject_metrics[class_subject_metrics['class'] == cls].set_index('subject')\n",
        "    filename = f\"{cls}_metrics.csv\"\n",
        "    class_df.to_csv(filename)\n",
        "    csv_files_to_zip.append(filename)\n",
        "    print(f\"  Saved: {filename}\")\n",
        "\n",
        "# Save overall metrics by subject\n",
        "print(\"\\nSaving overall metrics by subject...\")\n",
        "subject_metrics = metrics_df.groupby('subject')[metrics_columns].mean()\n",
        "output_filename_subject = \"overall_subject_metrics.csv\"\n",
        "subject_metrics.to_csv(output_filename_subject)\n",
        "csv_files_to_zip.append(output_filename_subject)\n",
        "print(f\"  Saved: {output_filename_subject}\")\n",
        "\n",
        "print(f\"\\nAll requested aggregated metrics files have been generated.\")\n",
        "\n",
        "# --- 4. Zip the generated CSV files ---\n",
        "zip_filename = \"class_subject_metrics.zip\"\n",
        "print(f\"\\nZipping all generated CSV files into '{zip_filename}'...\")\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for file in csv_files_to_zip:\n",
        "        if os.path.exists(file):\n",
        "            zf.write(file, os.path.basename(file))\n",
        "        else:\n",
        "            print(f\"Warning: File not found and skipped during zipping: {file}\")\n",
        "print(f\"\\nAll generated CSV files have been zipped into '{zip_filename}'.\")\n",
        "\n",
        "# --- 5. Display previews of the results ---\n",
        "print(\"\\n--- Example: Metrics for Class 1 (Subject-wise) ---\")\n",
        "# Check if 'Class1' exists in the data before trying to print\n",
        "if 'Class1' in class_subject_metrics['class'].unique():\n",
        "    class1_metrics = class_subject_metrics[class_subject_metrics['class'] == 'Class1'].set_index('subject')\n",
        "    print(class1_metrics.to_markdown(numalign=\"left\", stralign=\"left\", floatfmt=\".2f\"))\n",
        "else:\n",
        "    print(\"Class1 data not available in the input file for preview.\")\n",
        "\n",
        "print(\"\\n--- Overall Metrics by Subject ---\")\n",
        "print(subject_metrics.to_markdown(numalign=\"left\", stralign=\"left\", floatfmt=\".2f\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMbgOtzHqpzi",
        "outputId": "0d925090-82f5-4f7a-a31a-dbee942c6c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data loaded successfully.\n",
            "Calculating per-row metrics...\n",
            "\n",
            "Saving metrics for each class (subject-wise breakdown)...\n",
            "  Saved: Class1_metrics.csv\n",
            "  Saved: Class10_metrics.csv\n",
            "  Saved: Class2_metrics.csv\n",
            "  Saved: Class3_metrics.csv\n",
            "  Saved: Class4_metrics.csv\n",
            "  Saved: Class5_metrics.csv\n",
            "  Saved: Class6_metrics.csv\n",
            "  Saved: Class7_metrics.csv\n",
            "  Saved: Class8_metrics.csv\n",
            "  Saved: Class9_metrics.csv\n",
            "\n",
            "Saving overall metrics by subject...\n",
            "  Saved: overall_subject_metrics.csv\n",
            "\n",
            "All requested aggregated metrics files have been generated.\n",
            "\n",
            "Zipping all generated CSV files into 'class_subject_metrics.zip'...\n",
            "\n",
            "All generated CSV files have been zipped into 'class_subject_metrics.zip'.\n",
            "\n",
            "--- Example: Metrics for Class 1 (Subject-wise) ---\n",
            "| subject   | class   | Exact_Match   | BLEU   | rouge1_f1   | rouge2_f1   | rougel_f1   | Cosine_Similarity   |\n",
            "|:----------|:--------|:--------------|:-------|:------------|:------------|:------------|:--------------------|\n",
            "| english   | Class1  | 0.40          | 0.66   | 0.72        | 0.59        | 0.72        | 0.76                |\n",
            "| maths     | Class1  | 0.00          | 0.39   | 0.54        | 0.42        | 0.54        | 0.65                |\n",
            "| nepali    | Class1  | 0.00          | 0.45   | 0.50        | 0.29        | 0.45        | 0.79                |\n",
            "| social    | Class1  | 0.00          | 0.40   | 0.43        | 0.22        | 0.42        | 0.61                |\n",
            "\n",
            "--- Overall Metrics by Subject ---\n",
            "| subject   | Exact_Match   | BLEU   | rouge1_f1   | rouge2_f1   | rougel_f1   | Cosine_Similarity   |\n",
            "|:----------|:--------------|:-------|:------------|:------------|:------------|:--------------------|\n",
            "| english   | 0.12          | 0.42   | 0.48        | 0.32        | 0.45        | 0.51                |\n",
            "| maths     | 0.06          | 0.37   | 0.44        | 0.27        | 0.40        | 0.57                |\n",
            "| nepali    | 0.00          | 0.33   | 0.39        | 0.20        | 0.37        | 0.62                |\n",
            "| science   | 0.00          | 0.34   | 0.43        | 0.23        | 0.37        | 0.50                |\n",
            "| social    | 0.00          | 0.28   | 0.34        | 0.15        | 0.30        | 0.54                |\n"
          ]
        }
      ]
    }
  ]
}